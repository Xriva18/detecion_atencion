from google import genai
from google.genai import types
from core.config import settings
import json
import asyncio
import re
import os

class AIService:
    def __init__(self):
        print(f"[AIService] Inicializando servicio de IA con SDK Async...")
        self.client = None
        if settings.gemini_api_key:
            # Inicializamos el cliente. 
            # NOTA: En el nuevo SDK, no necesitamos configuraci√≥n extra para async,
            # se usa accediendo a .aio despu√©s.
            self.client = genai.Client(api_key=settings.gemini_api_key)
            print(f"[AIService] ‚úÖ Cliente Google GenAI configurado")
        else:
            print("[AIService] ‚ö†Ô∏è WARNING: GEMINI_API_KEY not set. AI features will not work.")

    async def _generate_with_retry(self, prompt: str, schema_config=None) -> str:
        """
        Funci√≥n interna que maneja los reintentos y el cambio de modelo.
        """
        if not self.client:
            raise Exception("Cliente IA no configurado")

        # Lista de modelos por prioridad. 
        # Usamos SOLO gemini-2.5-flash como solicit√≥ el usuario
        models_to_try = ["gemini-2.5-flash"]
        
        last_error = None

        for model_name in models_to_try:
            # AUMENTO DE INTENTOS: De 3 a 5 para mayor persistencia ante errores 503/429
            max_retries = 5
            for attempt in range(max_retries):
                try:
                    print(f"[AIService] üîÑ Intentando con {model_name} (Intento {attempt+1}/{max_retries})...")
                    
                    # Configuraci√≥n para respuesta JSON si se requiere
                    config = {}
                    if schema_config:
                        config['response_mime_type'] = 'application/json'
                        config['response_schema'] = schema_config

                    # LLAMADA AS√çNCRONA REAL (.aio)
                    response = await self.client.aio.models.generate_content(
                        model=model_name,
                        contents=prompt,
                        config=config
                    )
                    
                    if not response.text:
                        raise Exception("Respuesta vac√≠a de Gemini")

                    return response.text

                except Exception as e:
                    error_str = str(e)
                    last_error = e
                    print(f"[AIService] ‚ö†Ô∏è Error en {model_name} intento {attempt+1}: {e}")

                    # Si es error de cuota (429) o servidor sobrecargado (503)
                    if "429" in error_str or "RESOURCE_EXHAUSTED" in error_str or "503" in error_str:
                        # Backoff exponencial limitado: 5s, 10s, 20s, 30s, 30s...
                        wait_time = min(30, 5 * (2 ** attempt)) 
                        print(f"[AIService] ‚è≥ IA ocupada/cuota. Esperando {wait_time} segundos para reintentar...")
                        await asyncio.sleep(wait_time)
                    else:
                        # Si es otro tipo de error (ej. prompt invalido), no reintentamos este modelo
                        break 
            
            # Si llegamos aqu√≠, el modelo fall√≥ 3 veces, pasamos al siguiente modelo (1.5-flash)
            print(f"[AIService] ‚è≠Ô∏è Cambiando de modelo por fallos en {model_name}...")

        raise Exception(f"Fallaron todos los intentos y modelos. √öltimo error: {last_error}")

    async def summarize_text(self, text: str) -> str:
        """
        Generates a summary of the provided text.
        """
        if not self.client: return "Error: AI not configured."
        
        # Recortar texto para evitar errores de tokens masivos si el video es muy largo
        safe_text = text[:15000] 
        prompt = f"Por favor, proporciona un resumen conciso en espa√±ol del siguiente contenido de video:\n\n{safe_text}"
        
        try:
            result_text = await self._generate_with_retry(prompt)
            print(f"[AIService] ‚úÖ Resumen generado.")
            return result_text
        except Exception as e:
            print(f"[AIService] ‚ùå Error final generando resumen: {e}")
            return "No se pudo generar el resumen autom√°ticamente."

    async def generate_summary(self, text: str) -> str:
        """Alias para mantener compatibilidad con endpoint"""
        return await self.summarize_text(text)

    async def generate_quiz(self, text: str, attention_score: float, num_questions: int = 5) -> list:
        """
        Generates a quiz based on the text.
        """
        print(f"[AIService] üß† Iniciando generaci√≥n de quiz...")
        
        if not self.client:
            return [{"question": "Error IA", "options": ["Error"], "correct_answer": "Error"}]

        # Ajuste de dificultad
        if attention_score > 0.8:
            difficulty = "desafiante (an√°lisis)"
        elif attention_score > 0.5:
            difficulty = "moderada (comprensi√≥n)"
        else:
            difficulty = "simple (memoria)"

        prompt = f"""
        Act√∫a como un profesor experto y crea un examen profesional de {num_questions} preguntas de selecci√≥n m√∫ltiple en ESPA√ëOL.
        
        INSTRUCCIONES:
        1. Las preguntas deben basarse EXCLUSIVAMENTE en el contenido del siguiente texto (que es la transcripci√≥n de un video educativo).
        2. NO uses frases como "seg√∫n el texto", "en el fragmento", "el orador dice". En su lugar, usa frases naturales como "seg√∫n lo visto en el video", "en la clase", o simplemente formula la pregunta directamente.
        3. El tono debe ser formal, acad√©mico y desafiante, adecuado para un entorno universitario.
        4. Opciones de respuesta plausibles, evitando obviedades.
        
        Contexto del Estudiante:
        - Nivel de Atenci√≥n Detectado: {difficulty}
        - Transcripci√≥n del Video: 
        "{text[:25000]}" 

        FORMATO RESPUESTA (JSON RAW ARRAY):
        [
          {{
            "question": "¬øEnunciado de la pregunta?",
            "options": ["Opci√≥n A", "Opci√≥n B", "Opci√≥n C", "Opci√≥n D"],
            "correct_answer": "Opci√≥n Correcta"
          }}
        ]
        """

        try:
            # Usamos _generate_with_retry que maneja los 429 errors
            response_text = await self._generate_with_retry(prompt)
            
            # Limpieza agresiva del JSON por si el modelo incluye markdown
            cleaned_text = response_text.replace("```json", "").replace("```", "").strip()
            
            # A veces el modelo a√±ade texto antes o despu√©s, buscamos el array [...]
            match = re.search(r'\[.*\]', cleaned_text, re.DOTALL)
            if match:
                cleaned_text = match.group(0)

            quiz_data = json.loads(cleaned_text)
            print(f"[AIService] ‚úÖ Quiz generado correctamente con {len(quiz_data)} preguntas")
            return quiz_data

        except json.JSONDecodeError:
            print(f"[AIService] ‚ùå Error: La IA no devolvi√≥ un JSON v√°lido: {response_text[:100]}...")
            return [{"question": "Error de formato IA", "options": ["Reintentar"], "correct_answer": "Reintentar"}]
        except Exception as e:
            print(f"[AIService] ‚ùå Error fatal generando quiz: {e}")
            return [{"question": "Error de conexi√≥n con IA", "options": ["Reintentar"], "correct_answer": "Reintentar"}]

    async def generate_summary_from_video(self, video_path: str) -> str:
        """
        Sube un video a Gemini y genera un resumen usando el modelo multimodal.
        """
        print(f"[AIService] üé• Procesando video: {video_path}")
        if not self.client: return "Error: IA no configurada."

        try:
            # 1. Subir el archivo a Gemini
            print(f"[AIService] ‚¨ÜÔ∏è Subiendo video a Google AI ({os.path.getsize(video_path)} bytes)...")
            
            # CORRECCI√ìN: El argumento para archivos locales es 'path', no 'file'
            video_file = self.client.files.upload(
                path=video_path,
                config={'display_name': 'Video de Clase'}
            )
            
            print(f"[AIService] ‚úÖ Video subido: {video_file.name} (Estado: {video_file.state})")
            
            # 2. Esperar a que el video est√© procesado (ACTIVE)
            # Esperamos un m√°ximo de 60 segundos
            max_wait = 60
            waited = 0
            while video_file.state == "PROCESSING":
                print(f"[AIService] ‚è≥ Esperando procesamiento del video ({waited}s)...")
                await asyncio.sleep(2)
                waited += 2
                if waited > max_wait:
                    raise Exception("Tiempo de espera de procesamiento agotado")
                video_file = self.client.files.get(name=video_file.name)
            
            if video_file.state != "ACTIVE":
                raise Exception(f"Video no se proces√≥ correctamente. Estado: {video_file.state}")

            print(f"[AIService] ‚úÖ Video procesado y listo.")

            # 3. Generar contenido multimodal
            prompt = "Act√∫a como un profesor experto. Transcribe mentalmente el contenido de audio y visual de este video. Luego, genera un resumen educativo detallado en espa√±ol basado √öNICAMENTE en esa transcripci√≥n interna. Ignora cualquier intro o outro irrelevante. Estructura el resumen por puntos clave."
            
            # Usamos SOLO gemini-2.5-flash
            models = ["gemini-2.5-flash"]
            last_error = None
            
            for model in models:
                try:
                    print(f"[AIService] üîÑ Generando resumen con {model}...")
                    response = await self.client.aio.models.generate_content(
                        model=model, 
                        contents=[video_file, prompt]
                    )
                    if response.text:
                        print(f"[AIService] ‚úÖ Resumen de video generado.")
                        return response.text
                except Exception as e:
                    print(f"[AIService] ‚ö†Ô∏è Error con {model}: {e}")
                    last_error = e
                    if "429" in str(e):
                         await asyncio.sleep(10) # Wait a bit before next model
            
            raise last_error

        except Exception as e:
            print(f"[AIService] ‚ùå Error procesando video: {e}")
            return f"Error generando resumen del video: {e}"

ai_service = AIService()